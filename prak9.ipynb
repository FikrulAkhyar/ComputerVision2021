{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da8e67f",
   "metadata": {},
   "source": [
    "# Praktikum Visi Komputer\n",
    "## Modul ke-9\n",
    "\n",
    "## Object Tracking\n",
    "\n",
    "Pada modul ini, kita akan mempelajari tentang objeck tracking (pelacakan objek) dari live video. Kita akan\n",
    "mendiskusikan berbagai karakteristik yang dapat digunakan untuk melacak suatu objek. Kita juga akan belajar\n",
    "tentang berbagai metode dan teknik untuk pelacakan objek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b16f54",
   "metadata": {},
   "source": [
    "### 1. Frame differencing\n",
    "\n",
    "Frame differencing merupakam teknik paling sederhana yang dapat kita gunakan untuk melihat bagian video mana yang bergerak. Dari sebuah streaming video langsung, teknik frame differencing akan mengambil perbedaan antara frame yang berurutan dan menampilkan perbedaannya.\n",
    "\n",
    "Dengan menggunakan teknik frame differencing, hanya bagian yang bergerak dalam video yang disorot. Ini akan memberi informasi kepada kita untuk mengetahui area apa yang bergerak dalam video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9f1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Compute the frame difference\n",
    "def frame_diff(prev_frame, cur_frame, next_frame):\n",
    "    # Absolute difference between current frame and next frame\n",
    "    diff_frames1 = cv2.absdiff(next_frame, cur_frame)\n",
    "    \n",
    "    # Absolute difference between current frame and # previous frame\n",
    "    diff_frames2 = cv2.absdiff(cur_frame, prev_frame)\n",
    "    \n",
    "    # Return the result of bitwise 'AND' between the # above two resultant images\n",
    "    return cv2.bitwise_and(diff_frames1, diff_frames2)\n",
    "\n",
    "# Capture the frame from webcam\n",
    "def get_frame(cap):\n",
    "    # Capture the frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Resize the image\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Return the grayscale image\n",
    "    return cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    scaling_factor = 0.5\n",
    "    \n",
    "    prev_frame = get_frame(cap)\n",
    "    cur_frame = get_frame(cap)\n",
    "    next_frame = get_frame(cap)\n",
    "    # Iterate until the user presses the ESC key\n",
    "    while True:\n",
    "        # Display the result of frame differencing\n",
    "        cv2.imshow(\"Object Movement\", frame_diff(prev_frame, cur_frame, next_frame))\n",
    "        \n",
    "        # Update the variables\n",
    "        prev_frame = cur_frame\n",
    "        cur_frame = next_frame\n",
    "        next_frame = get_frame(cap)\n",
    "       \n",
    "    # Check if the user pressed ESC\n",
    "        key = cv2.waitKey(10)\n",
    "        if key == 27:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac3c674",
   "metadata": {},
   "source": [
    "### 2. Colorspace based tracking\n",
    "\n",
    "Frame differencing memberikan kita beberapa informasi yang berguna, tetapi tidak dapat digunakan ketika kita ingin menargetkan pelacakan hanya pada objek tertentu saja. Misal kita hanya ingin melacak sebuah objek yang berwarna biru saja. Dalam kasus ini, kita dapat menggunakan colorspace based tracking, dimana kita dapat menentukan target objek yang akan kita ingin lacak berdasarkan karakteristik warnanya. Untuk menggunakan teknik ini, kita perlu mengetahui distribusi warna dari objek yang menjadi target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319fcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Capture the input frame from webcam\n",
    "def get_frame(cap, scaling_factor):\n",
    "    # Capture the frame from video capture object\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Resize the input frame\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "if __name__=='__main__':\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    scaling_factor = 0.5\n",
    "\n",
    "    # Iterate until the user presses ESC key\n",
    "    while True:\n",
    "        frame = get_frame(cap, scaling_factor)\n",
    "\n",
    "        # Convert the HSV colorspace\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #Define 'blue' range in HSV colorspace\n",
    "        lower = np.array([60,100,100])\n",
    "        upper = np.array([180,255,255])\n",
    "        \n",
    "        #Define 'green' range in HSV colorspace\n",
    "        #lower = np.array([40, 40, 40])\n",
    "        #upper = np.array([70, 255, 255])\n",
    "              \n",
    "        # Threshold the HSV image to get only blue color\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "        # Bitwise-AND mask and original image\n",
    "        res = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "        res = cv2.medianBlur(res, 5)\n",
    "        cv2.imshow('Original image', frame)\n",
    "        cv2.imshow('Color Detector', res)\n",
    "\n",
    "        # Check if the user pressed ESC key\n",
    "        c = cv2.waitKey(5)\n",
    "        if c == 27:\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13a4e5",
   "metadata": {},
   "source": [
    "### 3. Membangun sebuah interactive object tracker\n",
    "\n",
    "Colorspace based tracking memberi kita kebebasan untuk melacak objek berdasarkan warna, tetapi pelacakan masih dibatasi oleh warna yang telah ditentukan sebelumnya. Bagaimana jika kita hanya ingin  melacak sebuah objek yang bebas kita pilih? Kita dapat meggunakan algoritma CAMShift (Continuously Adaptive Meanshift) yang merupakan perbaikan dari algoritma Meanshift.\n",
    "\n",
    "Algoritma Meanshift dapat memilih wilayah/region mana yang ingin dilacak dengan dengan sebuah bounding box. Dalam region tersebut akan dipilih sekelompok titik berdasarkan histogram warna dan dihitung centroid (titik pusat). Jika centroid terletak di pusat wilayah, kita tahu bahwa objek tidak bergerak. Tetapi jika centroid tidak berada di pusat wilayah, maka kita tahu bahwa objek bergerak ke suatu arah. \n",
    "\n",
    "Pergerakan centroid mengontrol arah kemana objek bergerak. Jadi, kita akan memindahkan bounding box kita ke lokasi baru sehingga centroid baru menjadi pusat bounding box ini. Oleh karena itu, algoritma ini disebut Meanshift karena digeser berdasarkan nilai mean (rata-rata). Dengan cara tesebut, pelacakan terus memperbarui keadaan sesuai dimana lokasi objek saat ini.\n",
    "\n",
    "Meanshift memiliki kekurangan, yaitu ukuran bounding box tidak dapat berubah. Saat objek jauh dari kamera, objek akan tampak kecil, begitu pun sebaliknya. Meanshift tidak memperhitungkan hal ini. Pada Meanshift, ukuran kotak pembatas akan tetap sama selama proses pelacakan. Oleh karena itu, diperkenalkan CAMShift. Kelebihan CAMShift adalah dapat menyesuaikan ukuran kotak pembatas dengan ukuran objek dan dapat melacak orientasi objek.\n",
    "\n",
    "Code di bawah ini akan menggunakan bounding ellipse untuk melacak sebuah benda yang kita pilih secara bebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a3278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ObjectTracker(object):\n",
    "    def __init__(self):\n",
    "        # Initialize the video capture object\n",
    "        # 0 -> indicates that frame should be captured\n",
    "        # from webcam\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "\n",
    "        # Capture the frame from the webcam\n",
    "        ret, self.frame = self.cap.read()\n",
    "\n",
    "        # Downsampling factor for the input frame\n",
    "        self.scaling_factor = 0.5\n",
    "        self.frame = cv2.resize(self.frame, None, fx=self.scaling_factor, fy=self.scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        cv2.namedWindow('Object Tracker')\n",
    "        cv2.setMouseCallback('Object Tracker', self.mouse_event)\n",
    "        self.selection = None\n",
    "        self.drag_start = None\n",
    "        self.tracking_state = 0\n",
    "    \n",
    "    # Method to track mouse events\n",
    "    def mouse_event(self, event, x, y, flags, param):\n",
    "        x, y = np.int16([x, y])\n",
    "\n",
    "        # Detecting the mouse button down event\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.drag_start = (x, y)\n",
    "            self.tracking_state = 0\n",
    "\n",
    "        if self.drag_start:\n",
    "            if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "                h, w = self.frame.shape[:2]\n",
    "                xo, yo = self.drag_start\n",
    "                x0, y0 = np.maximum(0, np.minimum([xo, yo], [x, y]))\n",
    "                x1, y1 = np.minimum([w, h], np.maximum([xo, yo], [x, y]))\n",
    "                self.selection = None\n",
    "\n",
    "                if x1-x0 > 0 and y1-y0 > 0:\n",
    "                    self.selection = (x0, y0, x1, y1)\n",
    "\n",
    "            else:\n",
    "                self.drag_start = None\n",
    "                if self.selection is not None:\n",
    "                    self.tracking_state = 1\n",
    "    \n",
    "    # Method to start tracking the object\n",
    "    def start_tracking(self):\n",
    "        # Iterate until the user presses the Esc key\n",
    "        while True:\n",
    "            # Capture the frame from webcam\n",
    "            ret, self.frame = self.cap.read()\n",
    "            \n",
    "            # Resize the input frame\n",
    "            self.frame = cv2.resize(self.frame, None, fx=self.scaling_factor, fy=self.scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "            vis = self.frame.copy()\n",
    "            \n",
    "            # Convert to HSV colorspace\n",
    "            hsv = cv2.cvtColor(self.frame, cv2.COLOR_BGR2HSV)\n",
    "            # Create the mask based on predefined thresholds.\n",
    "            mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
    "            \n",
    "            if self.selection:\n",
    "                x0, y0, x1, y1 = self.selection\n",
    "                self.track_window = (x0, y0, x1-x0, y1-y0)\n",
    "                hsv_roi = hsv[y0:y1, x0:x1]\n",
    "                mask_roi = mask[y0:y1, x0:x1]\n",
    "                \n",
    "                # Compute the histogram\n",
    "                hist = cv2.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n",
    "                \n",
    "                # Normalize and reshape the histogram\n",
    "                cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX);\n",
    "                self.hist = hist.reshape(-1)\n",
    "                vis_roi = vis[y0:y1, x0:x1]\n",
    "                cv2.bitwise_not(vis_roi, vis_roi)\n",
    "                vis[mask == 0] = 0\n",
    "            \n",
    "            if self.tracking_state == 1:\n",
    "                self.selection = None\n",
    "                # Compute the histogram back projection\n",
    "                prob = cv2.calcBackProject([hsv], [0], self.hist, [0, 180], 1)\n",
    "                prob &= mask\n",
    "                term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "                \n",
    "                # Apply CAMShift on 'prob'\n",
    "                track_box, self.track_window = cv2.CamShift(prob, self.track_window, term_crit)\n",
    "                \n",
    "                # Draw an ellipse around the object\n",
    "                cv2.ellipse(vis, track_box, (0, 255, 0), 2)\n",
    "            cv2.imshow('Object Tracker', vis)\n",
    "            \n",
    "            c = cv2.waitKey(5)\n",
    "            if c == 27:\n",
    "                break\n",
    "        cv2.destroyAllWindows()\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "        ObjectTracker().start_tracking()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6329add",
   "metadata": {},
   "source": [
    "### 4. Feature based tracking\n",
    "\n",
    "Feature based tracking mengacu pada pelacakan titik fitur individual di seluruh frame berturut-turut dalam video. Kita akan menggunakan teknik yang disebut optical flow untuk melacak fitur ini. Optical flow adalah salah satu teknik yang paling populer dalam computer vision. Kita akan memilih banyak poin fitur dan melacaknya melalui video.\n",
    "\n",
    "Dalam mendeteksi titik fitur, kita perlu menghitung vektor perpindahan dan menunjukkan gerakan titik-titik kunci tersebut di antara frame yang berurutan. Vektor ini disebut motion vector. Kita awali dengan mengekstrak titik fitur. Untuk setiap titik fitur, dibuat 3x3 patch dengan titik fitur di tengah. Asumsinya di sini adalah bahwa semua titik dalam setiap patch akan memiliki gerakan yang sama. Kita dapat menyesuaikan ukuran window sesuai dengan keperluan kita.\n",
    "\n",
    "Untuk setiap titik fitur dalam frame saat ini,  akan diambil 3x3 patch di sekitarnya sebagai titik referensi. Untuk patch ini, akan dilihat lingkungan frame sebelumnya untuk mendapatkan kecocokan terbaik. Lingkungan ini biasanya lebih besar dari 3x3 karena kita ingin mendapatkan patch yang paling dekat dengan patch yang sedang dipertimbangkan. Jalur dari piksel tengah dari patch yang cocok di frame sebelumnya ke piksel tengah patch yang dipertimbangkan dalam frame saat ini akan menjadi motion vector. Kita akan lakukan hal itu untuk semua fitur poin dan akan mengekstrak semua motion vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ad5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def start_tracking():\n",
    "    # Capture the input frame\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Downsampling factor for the image\n",
    "    scaling_factor = 0.5\n",
    "\n",
    "    # Number of frames to keep in the buffer when you\n",
    "    # are tracking. If you increase this number,\n",
    "    # feature points will have more \"inertia\"\n",
    "    num_frames_to_track = 5\n",
    "\n",
    "    # Skip every 'n' frames. This is just to increase the speed.\n",
    "    num_frames_jump = 2\n",
    "\n",
    "    tracking_paths = []\n",
    "    frame_index = 0\n",
    "    # 'winSize' refers to the size of each patch. These patches\n",
    "    # are the smallest blocks on which we operate and track\n",
    "    # the feature points. You can read more about the parameters\n",
    "    # here: http://goo.gl/ulwqLk\n",
    "    tracking_params = dict(winSize = (11, 11), maxLevel = 2, criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    # Iterate until the user presses the ESC key\n",
    "    while True:\n",
    "        # read the input frame\n",
    "        ret, frame = cap.read()\n",
    "        # downsample the input frame\n",
    "        frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        output_img = frame.copy()\n",
    "\n",
    "        if len(tracking_paths) > 0:\n",
    "            prev_img, current_img = prev_gray, frame_gray\n",
    "            feature_points_0 = np.float32([tp[-1] for tp in tracking_paths]).reshape(-1, 1, 2)\n",
    "\n",
    "            # Compute feature points using optical flow. You can\n",
    "            # refer to the documentation to learn more about the\n",
    "            # parameters here: http://goo.gl/t6P4SE\n",
    "            feature_points_1, _, _ = cv2.calcOpticalFlowPyrLK(prev_img, current_img, feature_points_0, None, **tracking_params)\n",
    "            feature_points_0_rev, _, _ = cv2.calcOpticalFlowPyrLK(current_img, prev_img, feature_points_1, None, **tracking_params)\n",
    "\n",
    "            # Compute the difference of the feature points\n",
    "            diff_feature_points = abs(feature_points_0-\n",
    "            feature_points_0_rev).reshape(-1, 2).max(-1)\n",
    "\n",
    "            # threshold and keep the good points\n",
    "            good_points = diff_feature_points < 1\n",
    "            new_tracking_paths = []\n",
    "\n",
    "            for tp, (x, y), good_points_flag in zip(tracking_paths, feature_points_1.reshape(-1, 2), good_points):\n",
    "                if not good_points_flag:\n",
    "                    continue\n",
    "                tp.append((x, y))\n",
    "\n",
    "                # Using the queue structure i.e. first in,\n",
    "                # first out\n",
    "                if len(tp) > num_frames_to_track:\n",
    "                    del tp[0]\n",
    "\n",
    "                new_tracking_paths.append(tp)\n",
    "\n",
    "                # draw green circles on top of the output image\n",
    "                cv2.circle(output_img, (int(x), int(y)), 3, (0, 255, 0), -1)\n",
    "\n",
    "            tracking_paths = new_tracking_paths\n",
    "\n",
    "            # draw green lines on top of the output image\n",
    "            cv2.polylines(output_img, [np.int32(tp) for tp in tracking_paths], False, (0, 150, 0))\n",
    "\n",
    "            # 'if' condition to skip every 'n'th frame\n",
    "        if not frame_index % num_frames_jump:\n",
    "            mask = np.zeros_like(frame_gray)\n",
    "            mask[:] = 255\n",
    "            for x, y in [np.int32(tp[-1]) for tp in tracking_paths]:\n",
    "                cv2.circle(mask, (x, y), 6, 0, -1)\n",
    "\n",
    "            # Extract good features to track. You can learn more\n",
    "            # about the parameters here: http://goo.gl/BI2Kml\n",
    "            feature_points = cv2.goodFeaturesToTrack(frame_gray, mask = mask, maxCorners = 500, qualityLevel = 0.3, minDistance = 7, blockSize = 7)\n",
    "            if feature_points is not None:\n",
    "                for x, y in np.float32(feature_points).reshape (-1, 2):\n",
    "                    tracking_paths.append([(x, y)])\n",
    "\n",
    "        frame_index += 1\n",
    "        prev_gray = frame_gray\n",
    "\n",
    "        cv2.imshow('Optical Flow', output_img)\n",
    "        # Check if the user pressed the ESC key\n",
    "        c = cv2.waitKey(1)\n",
    "        if c == 27:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_tracking()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3453181",
   "metadata": {},
   "source": [
    "### 5. Background subtraction\n",
    "\n",
    "Background subtraction sangat berguna dalam video surveillance (CCTV). Pada dasarnya, background subtraction bekerja dengan sangat baik untuk kasus-kasus di mana kita harus mendeteksi objek bergerak dalam pemandangan statis (tidak bergerak). Background subtraction ini bekerja dengan mendeteksi latar belakang dan menguranginya dengan frame saat ini untuk mendapatkan latar depan, yaitu objek bergerak. Untuk mendeteksi objek bergerak, kita perlu membuat model latar belakang terlebih dahulu.\n",
    "\n",
    "Ini tidak sama dengan frame differencing karena kita sebenarnya memodelkan latar belakang dan menggunakan model ini untuk mendeteksi objek bergerak. Jadi, background subtraction akan bekerja jauh lebih baik dari pada teknik frame differencing sederhana. Teknik background subtraction ini mencoba mendeteksi bagian statis dalam scene dan kemudian memasukkannya ke dalam model latar belakang. Jadi, ini adalah teknik adaptif yang dapat disesuaikan dengan scene. Intinya setiap bagian gambar yang statis secara bertahap akan menjadi bagian dari model latar belakang. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd8f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Capture the input frame\n",
    "def get_frame(cap, scaling_factor=0.5):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Resize the frame\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "    return frame\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Initialize the video capture object\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Create the background subtractor object\n",
    "    bgSubtractor = cv2.createBackgroundSubtractorKNN()\n",
    "    # This factor controls the learning rate of the algorithm.\n",
    "    # The learning rate refers to the rate at which your model\n",
    "    # will learn about the background. Higher value for\n",
    "    # 'history' indicates a slower learning rate.\n",
    "    #You can play with this parameter to see how it affects the output.\n",
    "    history = 100\n",
    "\n",
    "    # Iterate until the user presses the ESC key\n",
    "    while True:\n",
    "        frame = get_frame(cap, 0.5)\n",
    "        # Apply the background subtraction model to the # input frame\n",
    "        mask = bgSubtractor.apply(frame, learningRate=1.0/history)\n",
    "        \n",
    "            \n",
    "        # Convert from grayscale to 3-channel RGB\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "        cv2.imshow('Input frame', frame)\n",
    "        cv2.imshow('Moving Objects', mask & frame)\n",
    "\n",
    "        # Check if the user pressed the ESC key\n",
    "        c = cv2.waitKey(10)\n",
    "        if c == 27:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0758329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latihan \n",
    "#Cari satu paper terbaru dalam kurun waktu 5 tahun terakhir tentang object tracking\n",
    "#Baca dan pahami paper tersebut\n",
    "#Ringkas kesimpulannya dalam 3 alenia, 1 alenia minimal 3 kalimat.\n",
    "#Ringkasan kurang lebih akan menjawab:\n",
    "#Apa yang mereka kerjakan? Apa hal baru yang mereka temukan? Algoritma apa saja yang mereka pakai.\n",
    "#Sertakan link untuk men-download paper tersebut!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8df4d",
   "metadata": {},
   "source": [
    "# <center>Pendeteksian dan Pelacakan Objek Bergerak pada UAV berbasis Metode SUED</center>\n",
    "<center>Link Paper: <a href=\"http://ejnteti.jteti.ugm.ac.id/index.php/JNTETI/article/view/407\" target=\"_blank\">http://ejnteti.jteti.ugm.ac.id/index.php/JNTETI/article/view/407</a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e831bf1",
   "metadata": {},
   "source": [
    "<h3>Yang Dikerjakan</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c8c4a",
   "metadata": {},
   "source": [
    "Pada makalah ini dilakukan pendeteksian sebuah objek yang bergerak secara realtime. Awalnya dilakukan dengan menggunakan UAV atau drone, namun terdapat masalah dalam proses deteksi objek yang bergerak pada UAV yang disebut <strong>Uncertainly Constraint Factor(UCF)</strong> yaitu lingkungan, jenis objek, pencahayaan, kamera UAV, dan gerakan(motion). Karena hal tersebut pada makalah ini digunakanlah algoritma <strong>SUED</strong> untuk mengatasi masalah yang terjadi. Sehingga dengan menggunakan algoritma SUED dapat mengatasi noise yang disebabkan oleh UCF. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cc446",
   "metadata": {},
   "source": [
    "<h3>Hal baru yang ditemukan</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328dab17",
   "metadata": {},
   "source": [
    "Hasil simulasi algoritma SUED menunjukan bahwa objek bergerak dapat dideteksi berdasarkan gerakan yang membawa informasi piksel dari objek bergerak tersebut. Penggunaan kombinasi metode <strong>wavelet</strong> dan <strong>Operator Sobel</strong> menunjukkan adanya peningkatan unjuk kerja untuk algoritma SUED dan adanya penurunan nilai error antara hasil prediksi dan hasil pelacakan objek bergerak. Tetapi terdapat kelemahan dari penggunaan kombinasi metode wavelet dan operator sobel pada algoritma SUED yaitu tidak dapat mengatasi noise region yang memiliki area intensitas piksel yang besar yang mengakibatkan terjadinya kesalahan deteksi dan pelacakan objek bergerak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5462d",
   "metadata": {},
   "source": [
    "<h3>Algoritma yang dipakai</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661198d",
   "metadata": {},
   "source": [
    "Algoritma yang dipakai pada proses pendeteksian objek yang bergerak secara realtime ini adalah algoritma <strong> segmentation using edge based dilation(SUED)</strong>. Algoritma SUED ini mengombinasikan frame difference dan proses segmentasi secara bersama untuk mendapatkan hasil yang optimal. Algoritma SUED ini menggunakan wavelet dan sobel operator pada deteksi tepinya."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
